{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MLFLOW_TRACKING_URI\"] = \"https://dagshub.com/princevkurien/Bird_species_classfication.mlflow\"\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"princevkurien\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"]=\"7e5f50bcbd2a6865427b3e9be952d81eb8ec9ac5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"artifacts/training/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pydantic import BaseModel , FilePath , FileUrl , DirectoryPath , AnyUrl\n",
    "from pydantic.dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "class EvaluationConfig(BaseModel):\n",
    "    path_of_model: FilePath\n",
    "    evaluation_model_dir: DirectoryPath\n",
    "    test_data: DirectoryPath\n",
    "    best_model_path : Path\n",
    "    all_params: dict\n",
    "    mlflow_uri: str\n",
    "    score_dir : DirectoryPath\n",
    "    params_image_size: list\n",
    "    params_batch_size: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   root_dir:  artifacts/evaluation\n",
    "#   path_of_model: artifacts/training/model.h5\n",
    "#   evaluation_model_dir : artifacts/evaluation/model_dir \n",
    "#   test_data: artifacts/data_ingestion/test\n",
    "#   best_model_path : artifacts/evaluation/model_dir/best_model.h5\n",
    "#   mlflow_uri: \"https://dagshub.com/princevkurien/Bird_species_classfication.mlflow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BirdClassifier.constants import *\n",
    "from BirdClassifier.utils import create_directories, read_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self, config_file_path=CONFIG_FILE_PATH, param_file_path=PARAMS_FILE_PATH\n",
    "    ) -> None:\n",
    "        self.config = read_yaml(config_file_path)\n",
    "        self.params = read_yaml(param_file_path)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    def get_validation_config(self) -> EvaluationConfig:\n",
    "        \n",
    "        eval =self.config.evaluation\n",
    "        params = self.params\n",
    "        create_directories([eval.evaluation_model_dir, eval.score_dir])\n",
    "        eval_config = EvaluationConfig( path_of_model = eval.path_of_model,\n",
    "                                       test_data = eval.test_data , \n",
    "                                       score_dir=eval.score_dir ,\n",
    "                                       evaluation_model_dir = eval.evaluation_model_dir ,\n",
    "                                       best_model_path = eval.best_model_path , \n",
    "                                       mlflow_uri=eval.mlflow_uri,\n",
    "                                       all_params=params,\n",
    "                                       params_image_size=params.IMAGE_SIZE,\n",
    "                                       params_batch_size=self.params.BATCH_SIZE)\n",
    "        return eval_config\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BirdClassifier.utils import save_json , s3_download_model , upload_file , get_best_model_s3 , load_json\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "from urllib.parse import urlparse\n",
    "import time\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Evaluation:\n",
    "    def __init__(self, config: EvaluationConfig):\n",
    "        self.config = config\n",
    "        self.timestamp = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "    def _valid_generator(self):\n",
    "\n",
    "        datagenerator_kwargs = dict(\n",
    "            rescale = 1./255,\n",
    "        )\n",
    "\n",
    "        dataflow_kwargs = dict(\n",
    "            target_size=self.config.params_image_size[:-1],\n",
    "            batch_size=self.config.params_batch_size,\n",
    "            interpolation=\"bilinear\"\n",
    "        )\n",
    "\n",
    "        valid_datagenerator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "            **datagenerator_kwargs\n",
    "        )\n",
    "\n",
    "        self.valid_generator = valid_datagenerator.flow_from_directory(\n",
    "            directory=self.config.test_data,\n",
    "            **dataflow_kwargs\n",
    "        )\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(path: Path) -> tf.keras.Model:\n",
    "        return tf.keras.models.load_model(path)\n",
    "    @property\n",
    "    def _get_trained_model_path(self):\n",
    "        trained_model_name = f'model_{self.timestamp}.h5'\n",
    "        trained_model_path = os.path.join(self.config.evaluation_model_dir , trained_model_name)\n",
    "        shutil.copy(src= self.config.path_of_model , dst=trained_model_path)\n",
    "        return trained_model_path\n",
    "    \n",
    "\n",
    "\n",
    "    def evaluation(self):\n",
    "        self.train_model_path = self._get_trained_model_path\n",
    "        self.best_model_path = get_best_model_s3(self.config.best_model_path)\n",
    "        self.best_model = None\n",
    "        if self.best_model_path is not None:\n",
    "            self.best_model = self.load_model(self.best_model_path)\n",
    "        self.model = self.load_model(self.train_model_path)\n",
    "        self._valid_generator()\n",
    "        self.log_into_mlflow()\n",
    "        \n",
    "    def _evaluate_model(self , model):\n",
    "        scores = model.evaluate(self.valid_generator)\n",
    "        response_ = {\"loss\":scores[0], \"accuracy\": scores[1]}\n",
    "        return response_\n",
    "    def update_best_model(self ):\n",
    "        s3_model_result = self.result.s3_best_model_score\n",
    "        local_model_result = self.result.local_model_score\n",
    "        if (s3_model_result is None) :\n",
    "            pass \n",
    "        elif local_model_result.accuracy > s3_model_result.accuracy:\n",
    "            self.best_model = self.model\n",
    "            upload_file(file_name=self.train_model_path , object_name= \"Best_model\")\n",
    "            upload_file(file_name=self.best_model_path , object_name= f\"{self.timestamp}_model\")\n",
    "        else:\n",
    "            upload_file(file_name=self.train_model_path , object_name= f\"{self.timestamp}_model\")\n",
    "            \n",
    "        \n",
    "\n",
    "    def log_into_mlflow(self):\n",
    "        mlflow.set_registry_uri(self.config.mlflow_uri)\n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "        with mlflow.start_run():\n",
    "            mlflow.log_params(self.config.all_params)\n",
    "            self.scores_ = dict()\n",
    "            self.scores_[\"local_model_score\"] = self._evaluate_model(model=self.model)\n",
    "            self.scores_[\"s3_best_model_score\"] = None\n",
    "            if self.best_model is not None : \n",
    "                self.scores_[\"s3_best_model_score\"] = self._evaluate_model(model= self.best_model)\n",
    "            score_file_name= f'{self.timestamp}_score.json'\n",
    "            score_file_path = os.path.join(self.config.score_dir ,score_file_name)\n",
    "            save_json(path=Path(score_file_path) , data=self.scores_)\n",
    "            self.result = load_json(path=Path(score_file_path))\n",
    "            self.update_best_model()\n",
    "            local_model_result = self.result.local_model_score\n",
    "                \n",
    "            \n",
    "            mlflow.log_metrics(\n",
    "                {\"loss\": local_model_result.loss, \"accuracy\": local_model_result.accuracy}\n",
    "            )\n",
    "            # Model registry does not work with file store\n",
    "            if tracking_url_type_store != \"file\":\n",
    "\n",
    "                # Register the model\n",
    "                # There are other ways to use the Model Registry, which depends on the use case,\n",
    "                # please refer to the doc for more information:\n",
    "                # https://mlflow.org/docs/latest/model-registry.html#api-workflow\n",
    "                mlflow.keras.log_model(self.model, \"model\", registered_model_name=\"VGG16Model\")\n",
    "            else:\n",
    "                mlflow.keras.log_model(self.model, \"model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-30 15:40:00.652 | INFO     | BirdClassifier.utils.common:read_yaml:30 - yaml file: configs/config.yaml loaded successfully\n",
      "2022-09-30 15:40:00.653 | INFO     | BirdClassifier.utils.common:read_yaml:30 - yaml file: params.yaml loaded successfully\n",
      "2022-09-30 15:40:00.654 | INFO     | BirdClassifier.utils.common:create_directories:49 - created directory at: artifacts\n",
      "2022-09-30 15:40:00.655 | INFO     | BirdClassifier.utils.common:create_directories:49 - created directory at: artifacts/evaluation/model_dir\n",
      "2022-09-30 15:40:00.655 | INFO     | BirdClassifier.utils.common:create_directories:49 - created directory at: artifacts/evaluation/scores\n",
      "ERROR:root:list index out of range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 400 classes.\n",
      "63/63 [==============================] - 9s 144ms/step - loss: 0.9137 - accuracy: 0.7720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-30 15:40:12.462 | INFO     | BirdClassifier.utils.common:save_json:63 - json file saved at: artifacts/evaluation/scores/2022-09-30-15-40-00_score.json\n",
      "2022-09-30 15:40:12.463 | INFO     | BirdClassifier.utils.common:load_json:79 - json file loaded succesfully from: artifacts/evaluation/scores/2022-09-30-15-40-00_score.json\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 13). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpnbxi9x3d/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpnbxi9x3d/model/data/model/assets\n",
      "/home/pk/Desktop/Project/Bird_species_classfication/env/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "Successfully registered model ''.\n",
      "2022/09/30 15:41:54 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: VGG16Model, version 2\n",
      "Created version '2' of model 'VGG16Model'.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    val_config = config.get_validation_config()\n",
    "    evaluation = Evaluation(val_config)\n",
    "    evaluation.evaluation()\n",
    "    \n",
    "except Exception as e:\n",
    "   raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d9366193bdcd0ee3117fd525bc2129207b3bb8846f887ffdc700e03cbca0be1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
